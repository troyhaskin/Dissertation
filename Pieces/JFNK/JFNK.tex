% ===================================================================================== %
%                                        Header                                         %
% ===================================================================================== %
\documentclass[Prelim,12pt]{WisconsinThesis}

%\usepackage{bookmark}
\usepackage{import}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{algorithm2e}
\usepackage{environ}
\usepackage{esint}
\usepackage{multirow}

\input{HelperCommands}
\newcommand{\NeedReference}{\colorbox{yellow}{\textsc{need reference}}}
\DefaultFileName{Main}
\graphicspath{{./Graphics/}}

\let\bar\overline
\newcommand{\pdt}   {\partial_t\:\!}
\newcommand{\pdz}   {\partial_z}
\newcommand{\pdi}   {\partial_i}
\newcommand{\pdj}   {\partial_j}
\newcommand{\V}     {\ensuremath{\Omega}}
\newcommand{\dV}    {\,\partial\V}
\newcommand{\IntV}  {\int_{\V}}
\renewcommand{\S}   {\ensuremath{\Gamma}}
\newcommand{\dS}    {\,\partial\S}
\newcommand{\IntS}  {\int_{\S}}
\newcommand{\q}     {\ensuremath{q}}
\newcommand{\qi}    {\ensuremath{q_i}}

\newcommand{\by}    {\!\times\!}

\newcommand{\dx}    {\ensuremath{\Delta{x}}}


\DeclareMathOperator*{\Lim}{Lim}
\DeclareMathOperator*{\ArgMin}{ArgMin}
\DeclareMathOperator {\Cos}{Cos}
\DeclareMathOperator {\Sin}{Sin}
\DeclareMathOperator {\Span}{Span}

\setstretch{1.85}
\setlength{\parskip}{15pt}

\showthe\parskip

\begin{document}

\show\part
\show\chapter
\show\section

\chapter[JFNK Solver]{Jacobian-Free Newton-Krylov Solver}

A Jacobian-Free Newton-Krylov (JFNK) solver is a general name for an algorithm that solves a system of nonlinear equations.
As implied by the name, these solvers are characterized by two main features: lack of a need to form the exact Jacobian and Newton-like updates obtained from a Krylov method.
The need for a Jacobian and an explanation of Newton updates will be discussed first.
An overview of Krylov methods and the more specific Generalized Minimal Residual (GMRES) method will follow.
The section will conclude with how the pieces fit together to form the JFNK solver used in this work.





\section{Newton's Method}
Newton's method is an iterative technique that seeks to find a vector $x$ such that
\begin{equation}
    F(x) = 0
    \label{Eqn:NonlinearRootFind}
\end{equation}
where $x$ is an unknown $N \by 1$ vector and the function $F(x)$ is a given, nonlinear $N \by 1$ vector-valued function of $x$.
This problem is sometimes referred to as a multi-dimensional root-finding problem.
Because the function $F(x)$ is taken to be nonlinear, a closed-form solution to the equation may be impractical, if not impossible, to find even for a modest value of $N$.

In an effort to make the solution of \cref{Eqn:NonlinearRootFind} tractable, Newton's method aims to form solution from a recursive series of linear approximations and solves.
The method begins with some initial value of $x$ denoted as $x_0$.
If $F(x_0)$ is not sufficiently close to zero (see below), Newton's method seeks a better solution by computing a search direction $\Delta{x}$ from $x_0$ via a linearized (first-order Taylor expanded) version of $F(x_0 + \Delta{x})$.
The Taylor expansion of $F(x_0 + \Delta{x})$ about $x_0$ is
\begin{equation}
    F(x_0 + \Delta{x}) =  F(x_0) + \mathbb{J}_{F}(x_0)\Delta{x} + C(x_0,\Delta{x})
\end{equation}
where $\mathbb{J}_{F}(x_0)$ is an $N \by N$ Jacobian matrix and $C(x_0,\Delta{x})$ is an $N \by 1$ vector of higher-order corrections that enforce the equality.
To solve for $\Delta{x}$ using Newton's method, the higher-order corrections are taken to be identically zero; therefore, each row of the approximate Taylor expansion is an equation for the tangent plane of the associated element of $F(x)$ at the guess value $x_0$.
The search direction is then calculated from the intersections of all the tangent planes when they each equal zero; this can also be thought of as assuming the direction and magnitude of $\Delta{x}$ leads to the exact solution of the problem.
Using either interpretation, the resulting linear system used to solve for the unknown is
\begin{equation}
    \Delta{x} = - J_{F}^{-1}(x_0) F(x_0).
\end{equation}
Any search direction calculated using the above formula, regardless of the point of evaluation, is called a Newton update.

Of course, since the higher-order corrections were ignored in attaining the linear system, this $\Delta{x}$ will unlikely lead to the exact solution.
However, assuming $\Delta{x}$ leads to a new point that reduces any suitable norm of $F(x_0)$, it is arguably a better point and can hopefully return an even better approximation.
From the new point $x_1 = x_0 + \Delta{x}$, another linear system can be formed and the linearize-solve process above repeated ad infinitum.
Repeatedly updating the Taylor expansion point by solving a repeatedly updated linear system is the essence of Newton's method.
Starting from $x_0$, a sequence of linearly updated vectors $x_k$ is formed:
\begin{equation}
    x_k = x_{k-1} - \mathbb{J}_{F}^{-1}(x_{k-1}) F(x_{k-1})
\end{equation}
Ignoring certain complications and definitions, this update equation is the core of Newton's method.


\section{Krylov Methods}
Krylov methods are a class of techniques used to solve linear systems of the form
\begin{equation}
    A x = b
    \label{Eqn:BasicLinearProblem}
\end{equation}
where $x$ and $b$ are $N \by 1$ vectors and $A$ is an $N \by N$ nonsingular matrix.
Different methods place other limitations on $A$, but the most general condition (and the only condition required by GMRES) is that the coefficient matrix $A$ be invertible.
The exact solution to \cref{Eqn:BasicLinearProblem} is simply
\begin{equation}
    x = A^{-1} b.
    \label{Eqn:BasicLinearSolution}
\end{equation}
While trivially written symbolically, if $N$ is large, the exact inversion of $A$ can be computationally and memory intensive.
The computational cost of inverting $A$ is only exacerbated when solving nonlinear problems with a Newton-like scheme since several, if not many, linear solves are required to make one nonlinear advancement.



The solution in \cref{Eqn:BasicLinearSolution} will now be rewritten in a form that expresses the solution as a correction from some arbitrary initial guess and is inverse-free.
The new form is introduced to reflect how these methods are often presented in the literature, which often lack such a discussion.
Additionally, the form does lend itself well toward expressing the solution and error which accompany any Krylov method.



Given an initial guess vector $x_0$ with an associated residual vector $r_0 = b - A x_0$, a new vector $\tilde{x}$ can be created by shifting in a direction \dx{} off of the guess vector: $\tilde{x} = x_0 + \dx$.
If the new vector is not the solution to the problem, the shift induces a new residual $\tilde{r}$ of the form
\begin{equation}
    \tilde{r} = r_0  - A \dx.
    \label{Eqn:ResidualUpdateForm}
\end{equation}
Assuming $\tilde{x}$ is the true solution $x$, the induced residual $\tilde{r}$ is precisely $0$, and the shift is therefore $\dx = A^{-1}r_0$.
The solution can then be written in a guess-correction form:
\begin{equation}
    \tilde{x} = x_0 + A^{-1}r_0.
    \label{Eqn:SolutionResidualForm}
\end{equation}
The burden of calculating $A$'s inverse is still present.
To avoid this problem, we use a result of the Cayley-Hamilton theorem: the inverse of a square $N \by N$ matrix can be expressed as a linear combination of the matrix's powers from $0$ to $N-1$.
To wit, given the proper weights $c_i$, the following equality is satisfied: $A^{-1} = \sum c_i A^i$.
Substituting this expansion into \cref{Eqn:SolutionResidualForm} yields
\begin{equation}
    \tilde{x} = x_0 + \sum_{i = 0}^{N-1} c_i A^i r_0.
    \label{Eqn:ExactKrylovSolution}
\end{equation}
The true solution to the problem is now in a form that incorporates some initial guess and is free of matrix inversions.
The weights $c_i$, which are required for the equality to hold, are based on $A$'s characteristic polynomial and are, therefore, difficult to efficiently compute for large $N$.
In truth, any method that aims to find an exact solution will be hampered by computational complexity for large systems.



Krylov methods aim to mitigate computational deficiencies by finding an approximate solution.
The search for an approximate solution does not render all of the above work worthless but forms its heart.
In fact, the set of vectors $\{A^i r_0\}$ present in \cref{Eqn:ExactKrylovSolution} is called the Krylov subspace of $A$, denoted as $\mathcal{K}_N(A,r_0)$, and is the basis for all Krylov methods.
That set of vectors is of dimension $N$ and will hereafter be referred to as the full subspace.
Typically, the methods to be discussed use a variable number $m$ of lower dimensional spaces that will likewise be denoted as $\mathcal{K}_m(A,r_0)$.
The approximate solution formed by Krylov methods is based on one simplification to \cref{Eqn:ExactKrylovSolution}: only form as much of the full subspace as needed.
That is, the approximation is built from an iteratively grown set of chosen basis vectors that span the lower dimensions of the full subspace.
The maximum number of basis vectors needed varies by problem, but it is guaranteed that the approximation becomes better as the maximum increases.


Thus, letting $Z_m$ be an $N \by m$ matrix whose columns form a basis for $\mathcal{K}_m(A,r_0)$ and $w_m$ be an $m \by 1$ weight vector (analogous to the aforementioned $c_i$), the $m$-th approximate solution is
\begin{equation}
    x_m = x_0 + Z_m w_m.
    \label{Eqn:ApproximateKrylovSolution}
\end{equation}
A side effect of forming only a subset of the full Krylov subspace is that the weights are no longer based on the matrix's characteristic polynomial.
To remedy this problem, Krylov methods define the weights through some optimality condition placed upon the system at the current iteration.
Therefore, Krylov methods are defined by their choice of basis vectors and optimality condition.
The discussion of GMRES concretely covers both of these choices.



\section{GMRES}
GMRES was first proposed by Saad and Schultz in 1986.
The original formulation used the starting basis vector $ $


\subsection{Algorithm}


\section{Jacobian-Free Augmentation}


\end{document}

