% ===================================================================================== %
%                                        Header                                         %
% ===================================================================================== %
\documentclass[Prelim,12pt]{WisconsinThesis}

%\usepackage{bookmark}
\usepackage{import}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{algorithm2e}
\usepackage{environ}
\usepackage{esint}
\usepackage{multirow}

\input{HelperCommands}
\newcommand{\NeedReference}{\colorbox{yellow}{\textsc{need reference}}}
\DefaultFileName{Main}
\graphicspath{{./Graphics/}}

\let\bar\overline
\newcommand{\pdt}   {\partial_t\:\!}
\newcommand{\pdz}   {\partial_z}
\newcommand{\pdi}   {\partial_i}
\newcommand{\pdj}   {\partial_j}
\newcommand{\V}     {\ensuremath{\Omega}}
\newcommand{\dV}    {\,\partial\V}
\newcommand{\IntV}  {\int_{\V}}
\renewcommand{\S}   {\ensuremath{\Gamma}}
\newcommand{\dS}    {\,\partial\S}
\newcommand{\IntS}  {\int_{\S}}
\newcommand{\q}     {\ensuremath{q}}
\newcommand{\qi}    {\ensuremath{q_i}}

\newcommand{\by}    {\!\times\!}

\newcommand{\dx}    {\ensuremath{\Delta{x}}}


\DeclareMathOperator*{\Lim}{Lim}
\DeclareMathOperator*{\ArgMin}{ArgMin}
\DeclareMathOperator {\Cos}{Cos}
\DeclareMathOperator {\Sin}{Sin}
\DeclareMathOperator {\Span}{Span}

\setstretch{1.85}
\setlength{\parskip}{15pt}

\showthe\parskip

\begin{document}

\show\part
\show\chapter
\show\section

\chapter[JFNK Solver]{Jacobian-Free Newton-Krylov Solver}

A Jacobian-Free Newton-Krylov (JFNK) solver is a general name for an algorithm that solves a system of nonlinear equations.
As implied by the name, these solvers are characterized by two main features: lack of a need to form the exact Jacobian and Newton-like updates obtained from a Krylov method.
The need for a Jacobian and an explanation of Newton updates will be discussed first.
An overview of Krylov methods and the more specific Generalized Minimal Residual (GMRES) method will follow.
The section will conclude with how the pieces fit together to form the JFNK solver used in this work.





\section{Newton's Method}
Systems of coupled, nonlinear equations are pervasive across all disciplines of science and engineering.
They are also one of the most important and difficult class of problems to analyze and solve mathematically.
While there are numerous techniques for solving such problems, Newton's method is the singular option that will be explored in this section.

\subsection{Problem Statement}
The nonlinear problem under consideration is defined as follows: given an $N \by 1$ vector of algebraic, nonlinear equations $r(x)$, determine an $N \by 1$ vector $x^*$ such that
\begin{equation}
    r(x^*) = 0
    \label{Eqn:NonlinearRootFind}
\end{equation}
This problem is sometimes referred to as a multi-dimensional root-finding problem.
A different but mostly equivalent definition for the above problem is: given an $N \by 1$ vector of algebraic, nonlinear equations $r(x)$, determine an $N \by 1$ vector $x^*$ such that
\begin{equation}
    x^* = \ArgMin_{x} \|r(x)\|
    \label{Eqn:NonlinearMinimize}
\end{equation}
where $\|\cdot\|$ is any suitable norm for $r(x)$ and $\ArgMin_x$ is a function that returns the minimum possible value of its argument within the domain of $x$.
\Cref{Eqn:NonlinearRootFind} is preferred for the linearizations to be discussed while \cref{Eqn:NonlinearMinimize} is preferred for the stopping criteria of Newton's method also to be discussed.
Because the function $r(x)$ is taken to be nonlinear, a closed-form solution to either equation may be impractical, if not impossible, to find even for a modest value of $N$.
Therefore, numerical techniques, like Newton's method, are frequently used for solving the problem approximately.



\subsection{Newton Update}
In an effort to make the solution of \cref{Eqn:NonlinearRootFind} tractable, Newton's method aims to form a solution from a recursive series of linear approximations and solves.
The method begins with some initial value of $x$ denoted as $x_0$.
If $r(x_0)$ is not sufficiently close to zero (see below), Newton's method seeks a better solution by computing a search direction $\Delta{x}$ from $x_0$ via a linearized (first-order Taylor expanded) version of $r(x_0 + \Delta{x})$.
The Taylor expansion of $r(x_0 + \Delta{x})$ about $x_0$ is
\begin{equation}
    r(x_0 + \Delta{x}) =  r(x_0) + J_r(x_0)\Delta{x} + c(x_0,\Delta{x})
\end{equation}
where $J_r(x_0)$ is an $N \by N$ Jacobian matrix and $c(x_0,\Delta{x})$ is an $N \by 1$ vector of higher-order corrections that ensure the equality.
To solve for $\Delta{x}$ using Newton's method, the higher-order corrections are taken to be identically zero; therefore, each row of the approximate Taylor expansion is an equation for the tangent plane of the associated element of $r(x)$ at the guess value $x_0$.
The search direction is then calculated from the intersections of all the tangent planes when they each equal zero; this can also be thought of as assuming the direction and magnitude of $\Delta{x}$ leads to the exact solution of the problem.
Using either interpretation, the resulting linear system used to solve for the direction is
\begin{equation}
    \Delta{x} = - J_{r}^{-1}(x_0) r(x_0).
\end{equation}
Any search direction calculated using the above formula, regardless of the point of evaluation, is called a Newton update or a Newton direction.

Of course, since the higher-order corrections were ignored in attaining the linear system, this $\Delta{x}$ will unlikely lead to the exact solution.
However, assuming $\Delta{x}$ leads to a new point that reduces any suitable norm of $r(x_0)$, it is arguably a better point and can hopefully return an even better approximation.
From the new point $x_1 = x_0 + \Delta{x}$, another linear system can be formed and the above linearize-then-solve process repeated.

Repeatedly updating the Taylor expansion point by solving a repeatedly updated linear system is the essence of Newton's method.
Starting from $x_0$, a sequence of linearly updated vectors $x_k$ is formed:
\begin{equation}
    x_k = x_{k-1} - J_r^{-1}(x_{k-1}) r(x_{k-1})
\end{equation}
Setting aside algorithmic flow, stopping criteria for the recursion, and other items to be delineated, this update equation is Newton's method.

If the initial starting point $x_0$ is ``close'' to the true solution of the problem





\section{Krylov Methods}
Krylov methods are a class of techniques used to solve linear systems of the form
\begin{equation}
    A x = b
    \label{Eqn:BasicLinearProblem}
\end{equation}
where $x$ and $b$ are $N \by 1$ vectors and $A$ is an $N \by N$ nonsingular matrix.
Different methods place other limitations on $A$, but the most general condition (and the only condition required by GMRES) is that the coefficient matrix $A$ be invertible.
The exact solution to \cref{Eqn:BasicLinearProblem} is simply
\begin{equation}
    x = A^{-1} b.
    \label{Eqn:BasicLinearSolution}
\end{equation}
While trivially written symbolically, if $N$ is large, the exact inversion of $A$ can be computationally and memory intensive.
The computational cost of inverting $A$ is only exacerbated when solving nonlinear problems with a Newton-like scheme since several, if not many, linear solves are required to make one nonlinear advancement.

Krylov methods aim to provide an approximate solution to \cref{Eqn:BasicLinearProblem} without ever forming the inverse of $A$ explicitly.
To achieve this goal, first, the solution will be written in an inverse-free form.
Then, a general explanation of Krylov methods and Krylov subspaces will be given.
The section will conclude with the particular Krylov method used for this work: GMRES.



\subsection{Inverse-free Form}
Let $x_N$ represent the exact solution to \cref{Eqn:BasicLinearProblem} and $x_0$ represent an initial guess to the solution.
Assuming the problem is solvable, there exists some $\Delta{x}$ such that $x_N = x_0 + \Delta{x}$.
Therefore, the residual vector for the initial guess from the solution would be
\begin{equation}
    r_0 = b - A x_0
\end{equation}
Isolating $b$ on the left-hand side and multiplying through by $A^{-1}$ then gives
\begin{equation}
    A^{-1} b = x_0 + A^{-1} r_0
\end{equation}
Substituting this into \cref{Eqn:BasicLinearSolution} gives
\begin{equation}
    x_N = x_0 + A^{-1} r_0.
    \label{Eqn:SolutionResidualForm}
\end{equation}
The burden of calculating $A$'s inverse is still present.
To avoid this problem, we use a result of the Cayley-Hamilton theorem: the inverse of a square $N \by N$ matrix can be expressed as a linear combination of the matrix's powers from $0$ to $N-1$.
To wit, given the proper weights $c_i$, the following equality is satisfied: $A^{-1} = \sum c_i A^i$.
Substituting this expansion into \cref{Eqn:SolutionResidualForm} yields
\begin{equation}
    x_N = x_0 + \sum_{i = 0}^{N-1} c_i A^i r_0.
    \label{Eqn:ExactKrylovSolution}
\end{equation}
The true solution to the problem is now in a form that incorporates some initial guess and is free of matrix inversions.
However, the weights $c_i$, which are required for an equality to hold, are based on $A$'s characteristic polynomial.
Calculating these weights exactly are, as in the explicit calculation of the inverse, difficult to efficiently compute for large $N$ and an approximation must be made for tractability.


\subsection{General Method}

The set of vectors $\{A^i r_0\}$ present in \cref{Eqn:ExactKrylovSolution} is called the Krylov subspace of $A$, denoted as $\mathcal{K}_N(A,r_0)$, and is the basis for all Krylov methods.
The set is of dimension $N$ and will hereafter be referred to as the full subspace.
Typically, the methods to be discussed use a variable number $m$ of lower dimensional spaces that will likewise be denoted as $\mathcal{K}_m(A,r_0)$.
The approximate solution formed by Krylov methods is based on one simplification to \cref{Eqn:ExactKrylovSolution}: only form as much of the full subspace as needed.
That is, the approximation is built from an iteratively grown set of chosen basis vectors that span the lower dimensions of the full subspace.
The maximum number of basis vectors needed varies by problem, but it is guaranteed that the approximation will reach the true solution once the full subspace is formed (assuming exact arithmetic).


Letting $Z_m$ be an $N \by m$ matrix whose columns form a basis for $\mathcal{K}_m(A,r_0)$ and $w_m$ be an $m \by 1$ weight vector (analogous to the aforementioned $c_i$), the $m$-th approximate solution is
\begin{equation}
    x_m = x_0 + Z_m w_m.
    \label{Eqn:ApproximateKrylovSolution}
\end{equation}
A side effect of forming only a subset of the full Krylov subspace is that the weights are no longer based on the $A$'s characteristic polynomial.
To remedy this problem, Krylov methods define the weights through some optimality condition placed upon the system at the current iteration.
Therefore, Krylov methods are defined by their choice of basis vectors and optimality condition.
The optimality condition that allows for a least-squares solution of the $w_m$ vectors that decreases the total residual.
Once the residual is reduced below a selected tolerance, the current iterant is taken as the solution to the system.

Since Krylov methods are a family, a specific method arises from consideration of $A$'s structure, choice of Krylov basis vectors, and a choice of optimality condition.
GMRES will give concreteness to these three choices.


\subsection{GMRES}
GMRES was first proposed by Saad and Schultz in 1986.
The original formulation used the starting basis vector $ $


\subsection{Algorithm}


\section{Jacobian-Free Augmentation}


\end{document}

